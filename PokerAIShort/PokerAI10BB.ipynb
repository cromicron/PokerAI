{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9431e6d4-b0d7-4bf1-a845-59012120ccdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-13 19:35:00.391611: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-13 19:35:00.391654: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-01-13 19:35:03.788458: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-01-13 19:35:03.788487: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-01-13 19:35:03.788510: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cromi-Lenovo-V15-ADA): /proc/driver/nvidia/version does not exist\n",
      "2022-01-13 19:35:03.788730: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from Agent2 import Agent2, ReplayBuffer2\n",
    "from Agent import Agent, ReplayBuffer\n",
    "from PokerGame import HUPoker\n",
    "from PokerInterface import PokerInterface, play_against\n",
    "from TransformVillain import transformStateVillain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "625467e1-0f0a-42d3-980f-d45525538605",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "gamma = 1\n",
    "n_actions = 12\n",
    "epsilon = 1\n",
    "batch_size = 32\n",
    "input_dims = (102,)\n",
    "stacksize = 10\n",
    "fname = 'trainedModels/PokerAI10BB.h5'\n",
    "layerSizes = (256,128,64, 32, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9ff1db9-4fdd-4b04-89cd-c60af63c69d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "newAgent = Agent2(lr, gamma, n_actions, 0.2, batch_size, input_dims, stacksize, fname = fname, handEval=True, layerSizes=layerSizes,mem_size=100000)\n",
    "oldAgent = Agent2(lr, gamma, n_actions, 0, batch_size, input_dims, stacksize, fname = fname, handEval=True, layerSizes=layerSizes,mem_size=0)\n",
    "newAgent.load_model()\n",
    "oldAgent.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "661f865a-f47f-4649-95eb-8b268bbe7170",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_hero = stacksize\n",
    "stack_villain = stacksize\n",
    "agentHero = newAgent\n",
    "agentVillain = oldAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dfda127-0de2-4604-b9a6-fc34b73edef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = HUPoker(stack_hero, stack_villain, agentHero, agentVillain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2375c19f-c29c-4387-80c4-d1551c5518e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal hand by hand approach in learning\n",
    "import numpy as np\n",
    "\n",
    "n_rounds = 1001\n",
    "score = 0\n",
    "for i in range(n_rounds):\n",
    "\n",
    "    done =True\n",
    "\n",
    "    while done:\n",
    "        observation, done = game.reset()\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        error = False\n",
    "        action = agentHero.choose_action(observation)\n",
    "        observation = np.append(observation,[game.pwinHero, game.plooseHero, game.pwinAvg, game.plooseAvg, game.stdWinAvg])\n",
    "\n",
    "        try: \n",
    "            done = game.step(action) #sometimes the chosen actions lead to too many moves on a street (over 20) This happens when players both chose to engage in minraise battles.\n",
    "                                    #we want to severely punish this behavior.\n",
    "        except IndexError: #error happens when more than 20 actions are attepted on a single street.\n",
    "            done = True\n",
    "            error = True\n",
    "\n",
    "        observation_ = game.observation #if street changed from \n",
    "        observation_ = np.append(observation_,[game.pwinHero, game.plooseHero, game.pwinAvg, game.plooseAvg, game.stdWinAvg])\n",
    "        if not done:\n",
    "            reward = 0\n",
    "        else:\n",
    "            if error:\n",
    "                reward = -stacksize\n",
    "            else:\n",
    "                reward = 0.5+game.stack_sb - game.starting_hero if game.position == 0 else 1+game.stack_bb - game.starting_hero #to take into account the blinds are posted anyway regardless of action.\n",
    "        score += reward - 0.5 if game.position == 0 else reward - 1\n",
    "        avgScore = round(score//(i+0.00001),2)\n",
    "        agentHero.store_transition(observation, action, reward, observation_, done)\n",
    "        observation = game.observation\n",
    "        \n",
    "        \n",
    "        loss = agentHero.learn()\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    print(\"hands played: \", i+1, \"reward round:\", reward, \"total score:\", score, \"avg score\", avgScore, \"epsilon\", agentHero.epsilon)\n",
    "    if i %10 ==0:\n",
    "        states, actions, rewards, states_, dones = agentHero.memory.sample_buffer(2)\n",
    "        q_eval = agentHero.q_eval.predict(states)\n",
    "        pos = states[0:1][0][0]\n",
    "        street = states[0:1][0][3]\n",
    "        hand = (states[0:1][0][4],states[0:1][0][10],states[0:1][0][5],states[0:1][0][11])\n",
    "        pot = states[0:1][0][7]\n",
    "        board = states[0:1][0][-15:-5]\n",
    "        print(\"pos\", pos, \"street\", street)\n",
    "        print(\"hand\", hand)\n",
    "        print(\"pot\", pot)\n",
    "        print(\"board\",board)\n",
    "        print(q_eval[0:1])\n",
    "\n",
    "\n",
    "    if i%1000 == 0:\n",
    "        #agentHero.save_model()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fbc1acc-423c-4b4a-8662-c3aa585d2c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 8 engines with <class 'ipyparallel.cluster.launcher.LocalEngineSetLauncher'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Cluster(cluster_id='1642098916-v8xd', profile='default', controller=<running>, engine_sets=['1642098917'])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "cluster = ipp.Cluster(n=8)\n",
    "await cluster.start_cluster() # or cluster.start_cluster_sync() without await"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e3d756e-f718-48e4-bc50-6bbb6331f48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 8/8 [00:03<00:00,  2.07engine/s]\n"
     ]
    }
   ],
   "source": [
    "rc = cluster.connect_client_sync()\n",
    "rc.wait_for_engines(n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddc79fc6-be6b-4844-ae0f-aac9aa683ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-13 19:35:28.125265: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f0635e33-2650-494d-bc1c-b77f19daf462/assets\n",
      "INFO:tensorflow:Assets written to: ram://1c9e41cb-3a3f-43ff-a0cb-4594250b6c30/assets\n",
      "INFO:tensorflow:Assets written to: ram://f28bfa1e-90ce-49a5-8127-aeedc75a1033/assets\n",
      "INFO:tensorflow:Assets written to: ram://d9d91878-3a09-481c-b4cf-7749a5596e7f/assets\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import pickle\n",
    "fname = 'trainedModels/PokerAI10BB.h5'\n",
    "layerSizes = (256,128,64, 32, 16)\n",
    "\n",
    "nDec = 10\n",
    "lr = 0.00001\n",
    "epsilon = 0# not meaningful for agentLearn, because he is not playing against anyone. Still all parameters must be included in agent.\n",
    "batch_size = 16\n",
    "input_dims = (102,)\n",
    "stacksize = 10\n",
    "nCores = 8\n",
    "agentLearn = Agent2(gamma = gamma, n_actions=n_actions,lr = lr, batch_size = batch_size, input_dims=input_dims, stacksize=stacksize, epsilon = 0, fname = fname, layerSizes=layerSizes, mem_size = nDec*nCores)\n",
    "\n",
    "try:\n",
    "    agentLearn.load_model()\n",
    "    print('Model loaded')\n",
    "except:\n",
    "    print('no Model loaded')\n",
    "K.set_value(agentLearn.q_eval.optimizer.learning_rate, lr)\n",
    "weightsHero = agentLearn.q_eval.get_weights()\n",
    "\n",
    "agentHero= Agent2(lr = 0.1, gamma=1, n_actions=12,epsilon=0.1, batch_size=16,\n",
    "        input_dims=(102,), stacksize =stacksize, \n",
    "        mem_size=nDec, \n",
    "        epsilon_end=0.05,\n",
    "        fname=fname, handEval=True,layerSizes=layerSizes)\n",
    "agent_vil= Agent2(lr = 0.1, gamma=1, n_actions=12,epsilon=0, batch_size=16,\n",
    "    input_dims=(102,), stacksize =stacksize, \n",
    "    mem_size=2, \n",
    "    epsilon_end=0.05,\n",
    "    fname=fname,layerSizes=layerSizes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = HUPoker(stacksize, stacksize, agentHero, agent_vil)\n",
    "rc[:]['agentHero']=agentHero\n",
    "rc[:]['agent_vil']=agent_vil\n",
    "rc[:]['weightsHero']=weightsHero\n",
    "\n",
    "#We want a diversity of opponents in the environment to prevent overfitting, and only update the weights of Villains from time to time, to keep the environment stable enough.\n",
    "#The current weights for villain are saved in the weights file.\n",
    "try:\n",
    "    with open(\"weightsVil10BB.txt\", \"rb\") as weightsFile:   # Unpickling\n",
    "        newWeights = pickle.load(weightsFile)\n",
    "    for i in range(len(newWeights)):\n",
    "        rc[i]['weightsVil']=newWeights[i]\n",
    "\n",
    "except:\n",
    "    rc[:]['weightsVil']=weightsHero\n",
    "    print(\"exception!\")\n",
    "\n",
    "\n",
    "rc[:]['ReplayBuffer2']= ReplayBuffer2\n",
    "rc[:]['env']= env\n",
    "rc[:]['nDec'] = nDec\n",
    "rc[:]['stacksize']=stacksize\n",
    "\n",
    "\n",
    "def play(instance):\n",
    "    import numpy as np\n",
    "\n",
    "    agentHero.q_eval.set_weights(weightsHero)\n",
    "    agent_vil.q_eval.set_weights(weightsVil)\n",
    "    n_rounds = 1\n",
    "    env.reset()\n",
    "\n",
    "    for i in range(n_rounds):\n",
    "        agentHero.memory = ReplayBuffer2(nDec, (107,))\n",
    "\n",
    "        while agentHero.memory.mem_cntr < nDec:\n",
    "\n",
    "            done =True\n",
    "\n",
    "            while done:\n",
    "                observation, done = env.reset()\n",
    "                hand = env.holecards\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                error = False\n",
    "                action = agentHero.choose_action(observation)\n",
    "                observation = np.append(observation,[env.pwinHero, env.plooseHero, env.pwinAvg, env.plooseAvg, env.stdWinAvg])\n",
    "\n",
    "                try: \n",
    "                    done = env.step(action) #sometimes the chosen actions lead to too many moves on a street (over 20) This happens when players both chose to engage in minraise battles.\n",
    "                                            #we want to severely punish this behavior.\n",
    "                except IndexError: #error happens when more than 20 actions are attepted on a single street.\n",
    "                    done = True\n",
    "                    error = True\n",
    "\n",
    "                observation_ = env.observation\n",
    "                observation_ = np.append(observation_,[env.pwinHero, env.plooseHero, env.pwinAvg, env.plooseAvg, env.stdWinAvg])\n",
    "                if not done:\n",
    "                    reward = 0\n",
    "                else:\n",
    "                    if error:\n",
    "                        reward = -stacksize\n",
    "                    else:\n",
    "                        reward = 0.5+env.stack_sb - env.starting_hero if env.position == 0 else 1+env.stack_bb - env.starting_hero #to take into account the blinds are posted anyway regardless of action.\n",
    "\n",
    "                agentHero.store_transition(observation, action, reward, observation_, done)\n",
    "                    #If the agent is playing against himself to learn, we can use villains play to train also. Everything except for villains hand can be derived from the observation. So we only have to save villains cards.\n",
    "\n",
    "                observation = env.observation\n",
    "\n",
    "\n",
    "        memoryDict = {'states': agentHero.memory.state_memory[:nDec], 'states_': agentHero.memory.new_state_memory[:nDec], \n",
    "                      'rewards':agentHero.memory.reward_memory[:nDec], 'actions': agentHero.memory.action_memory[:nDec], \n",
    "                      'terminals': agentHero.memory.terminal_memory[:nDec]}\n",
    "\n",
    "        agentHero.memory.mem_cntr =0\n",
    "    return memoryDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c14166-4d5b-487d-91b8-7996131084cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "nCores = 8#how many parallel cores were running.\n",
    "oldWeightsCount = 1\n",
    "\n",
    "for j in range (5000):\n",
    "    print (j)\n",
    "    memoryList = rc[:].map_sync(play,range(nCores))\n",
    "\n",
    "    states = []\n",
    "    states_=[]\n",
    "    rewards=[]\n",
    "    actions=[]\n",
    "    terminals=[]\n",
    "    \n",
    "    for i in range(nCores):\n",
    "        states.append(memoryList[i]['states'])\n",
    "        states_.append(memoryList[i]['states_'])\n",
    "        rewards.append(memoryList[i]['rewards'])\n",
    "        actions.append(memoryList[i]['actions'])\n",
    "        terminals.append(memoryList[i]['terminals'])\n",
    "    \n",
    "    state_memory=np.concatenate(states,axis=0)\n",
    "    new_state_memory = np.concatenate(states_,axis=0)\n",
    "    reward_memory = np.concatenate(rewards,axis=0)\n",
    "    action_memory = np.concatenate(actions,axis=0)\n",
    "    terminal_memory = np.concatenate(terminals,axis=0)\n",
    "    \n",
    "    agentLearn.memory.state_memory = state_memory\n",
    "    agentLearn.memory.new_state_memory = new_state_memory\n",
    "    agentLearn.memory.reward_memory = reward_memory\n",
    "    agentLearn.memory.action_memory = action_memory\n",
    "    agentLearn.memory.terminal_memory = terminal_memory\n",
    "    agentLearn.memory.mem_cntr = nCores*nDec\n",
    "    \n",
    "    r1 = np.random.randint(0, nCores*nDec)\n",
    "    r2 = np.random.randint(0, nCores*nDec)\n",
    "    observation = agentLearn.memory.state_memory[r1]\n",
    "    print(observation[0:6],agentLearn.q_eval.predict(np.array([observation])))\n",
    "    observation = agentLearn.memory.state_memory[r2]\n",
    "    print(observation[0:6],agentLearn.q_eval.predict(np.array([observation])))\n",
    "\n",
    " \n",
    "    agentLearn.learnMass()   \n",
    "    agentLearn.save_model()\n",
    "    \n",
    "    gc.collect()\n",
    "    weights = agentLearn.q_eval.get_weights()\n",
    "    rc[:]['weightsHero']=weights #send current weights of heros model to the engines.\n",
    "    rc[7]['weightsVillain']=weights #one opponent always has the current weights.\n",
    "    #update villain weights. After some steps, one of the villain networks gets updated.\n",
    "    frequencyUpdateVillain = 200\n",
    "    \n",
    "    if j%frequencyUpdateVillain ==0:\n",
    "        print(\"exchanging weights of\",oldWeightsCount)\n",
    "        print (rc[oldWeightsCount]['weightsVil'][0])\n",
    "        rc[oldWeightsCount]['weightsVil']= weights\n",
    "        print (rc[oldWeightsCount]['weightsVil'][0])\n",
    "        if oldWeightsCount == 6:\n",
    "            oldWeightsCount = 0\n",
    "        else:\n",
    "            oldWeightsCount+=1\n",
    "\n",
    "#save villain weights\n",
    "weightsVil = rc[:]['weightsVil']\n",
    "with open(\"weightsVil10BB.txt\", \"wb\") as weightsFile:   #Pickling\n",
    "    pickle.dump(weightsVil, weightsFile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfc30b3a-f83c-4a47-ac3b-3928066afbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save villain weights\n",
    "weightsVil = rc[:]['weightsVil']\n",
    "with open(\"weightsVil10BB.txt\", \"wb\") as weightsFile:   #Pickling\n",
    "    pickle.dump(weightsVil, weightsFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17c6950-fc81-4d9c-9485-1ae7b84c3035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
