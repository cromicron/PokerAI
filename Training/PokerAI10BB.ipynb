{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9431e6d4-b0d7-4bf1-a845-59012120ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from Agent2 import Agent2, ReplayBuffer2\n",
    "from Agent import Agent, ReplayBuffer\n",
    "from game.PokerGame import HUPoker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "625467e1-0f0a-42d3-980f-d45525538605",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "gamma = 1\n",
    "n_actions = 12\n",
    "epsilon = 1\n",
    "batch_size = 32\n",
    "input_dims = (102,)\n",
    "stacksize = 10\n",
    "fname = 'trainedModels/PokerAI10BB.h5'\n",
    "layerSizes = (256,128,64, 32, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9ff1db9-4fdd-4b04-89cd-c60af63c69d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "newAgent = Agent2(lr, gamma, n_actions, 0.2, batch_size, input_dims, stacksize, fname = fname, handEval=True, layerSizes=layerSizes,mem_size=100000)\n",
    "oldAgent = Agent2(lr, gamma, n_actions, 0, batch_size, input_dims, stacksize, fname = fname, handEval=True, layerSizes=layerSizes,mem_size=0)\n",
    "newAgent.load_model()\n",
    "oldAgent.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "661f865a-f47f-4649-95eb-8b268bbe7170",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_hero = stacksize\n",
    "stack_villain = stacksize\n",
    "agentHero = newAgent\n",
    "agentVillain = oldAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dfda127-0de2-4604-b9a6-fc34b73edef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = HUPoker(stack_hero, stack_villain, agentHero, agentVillain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2375c19f-c29c-4387-80c4-d1551c5518e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal hand by hand approach in learning\n",
    "import numpy as np\n",
    "\n",
    "n_rounds = 1001\n",
    "score = 0\n",
    "for i in range(n_rounds):\n",
    "\n",
    "    done =True\n",
    "\n",
    "    while done:\n",
    "        observation, done = game.reset()\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        error = False\n",
    "        action = agentHero.choose_action(observation)\n",
    "        observation = np.append(observation,[game.pwinHero, game.plooseHero, game.pwinAvg, game.plooseAvg, game.stdWinAvg])\n",
    "\n",
    "        try: \n",
    "            done = game.step(action) #sometimes the chosen actions lead to too many moves on a street (over 20) This happens when players both chose to engage in minraise battles.\n",
    "                                    #we want to severely punish this behavior.\n",
    "        except IndexError: #error happens when more than 20 actions are attepted on a single street.\n",
    "            done = True\n",
    "            error = True\n",
    "\n",
    "        observation_ = game.observation #if street changed from \n",
    "        observation_ = np.append(observation_,[game.pwinHero, game.plooseHero, game.pwinAvg, game.plooseAvg, game.stdWinAvg])\n",
    "        if not done:\n",
    "            reward = 0\n",
    "        else:\n",
    "            if error:\n",
    "                reward = -stacksize\n",
    "            else:\n",
    "                reward = 0.5+game.stack_sb - game.starting_hero if game.position == 0 else 1+game.stack_bb - game.starting_hero #to take into account the blinds are posted anyway regardless of action.\n",
    "        score += reward - 0.5 if game.position == 0 else reward - 1\n",
    "        avgScore = round(score//(i+0.00001),2)\n",
    "        agentHero.store_transition(observation, action, reward, observation_, done)\n",
    "        observation = game.observation\n",
    "\n",
    "\n",
    "        loss = agentHero.learn()\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    print(\"hands played: \", i+1, \"reward round:\", reward, \"total score:\", score, \"avg score\", avgScore, \"epsilon\", agentHero.epsilon)\n",
    "    if i %10 ==0:\n",
    "        states, actions, rewards, states_, dones = agentHero.memory.sample_buffer(2)\n",
    "        q_eval = agentHero.q_eval.predict(states)\n",
    "        pos = states[0:1][0][0]\n",
    "        street = states[0:1][0][3]\n",
    "        hand = (states[0:1][0][4],states[0:1][0][10],states[0:1][0][5],states[0:1][0][11])\n",
    "        pot = states[0:1][0][7]\n",
    "        board = states[0:1][0][-15:-5]\n",
    "        print(\"pos\", pos, \"street\", street)\n",
    "        print(\"hand\", hand)\n",
    "        print(\"pot\", pot)\n",
    "        print(\"board\",board)\n",
    "        print(q_eval[0:1])\n",
    "\n",
    "\n",
    "    #if i%1000 == 0:\n",
    "      #  agentHero.save_model()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fbc1acc-423c-4b4a-8662-c3aa585d2c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 8 engines with <class 'ipyparallel.cluster.launcher.LocalEngineSetLauncher'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Cluster(cluster_id='1649242437-vlh6', profile='default', controller=<running>, engine_sets=['1649242438'])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "cluster = ipp.Cluster(n=8)\n",
    "await cluster.start_cluster() # or cluster.start_cluster_sync() without await"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e3d756e-f718-48e4-bc50-6bbb6331f48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 8/8 [00:00<00:00, 12.29engine/s]\n"
     ]
    }
   ],
   "source": [
    "rc = cluster.connect_client_sync()\n",
    "rc.wait_for_engines(n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddc79fc6-be6b-4844-ae0f-aac9aa683ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 12:55:21.692956: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://5506eb25-2982-44ac-9ff0-f2521837c9e9/assets\n",
      "INFO:tensorflow:Assets written to: ram://97b44d6b-c87d-453c-b87e-d8173d77a574/assets\n",
      "INFO:tensorflow:Assets written to: ram://bcded2e0-a03a-4773-9a02-229e675423d3/assets\n",
      "INFO:tensorflow:Assets written to: ram://488e6407-8322-4499-aa0e-888eda497662/assets\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import pickle\n",
    "fname = 'trainedModels/PokerAI10BB.h5'\n",
    "layerSizes = (256,128,64, 32, 16)\n",
    "\n",
    "nDec = 10\n",
    "lr = 0.00001\n",
    "epsilon = 0# not meaningful for agentLearn, because he is not playing against anyone. Still all parameters must be included in agent.\n",
    "batch_size = 16\n",
    "input_dims = (102,)\n",
    "stacksize = 10\n",
    "nCores = 8\n",
    "agentLearn = Agent2(gamma = gamma, n_actions=n_actions,lr = lr, batch_size = batch_size, input_dims=input_dims, stacksize=stacksize, epsilon = 0, fname = fname, layerSizes=layerSizes, mem_size = nDec*nCores)\n",
    "\n",
    "try:\n",
    "    agentLearn.load_model()\n",
    "    print('Model loaded')\n",
    "except:\n",
    "    print('no Model loaded')\n",
    "K.set_value(agentLearn.q_eval.optimizer.learning_rate, lr)\n",
    "weightsHero = agentLearn.q_eval.get_weights()\n",
    "\n",
    "agentHero= Agent2(lr = 0.1, gamma=1, n_actions=12,epsilon=0.1, batch_size=16,\n",
    "        input_dims=(102,), stacksize =stacksize, \n",
    "        mem_size=nDec, \n",
    "        epsilon_end=0.05,\n",
    "        fname=fname, handEval=True,layerSizes=layerSizes)\n",
    "agent_vil= Agent2(lr = 0.1, gamma=1, n_actions=12,epsilon=0, batch_size=16,\n",
    "    input_dims=(102,), stacksize =stacksize, \n",
    "    mem_size=2, \n",
    "    epsilon_end=0.05,\n",
    "    fname=fname,layerSizes=layerSizes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = HUPoker(stacksize, stacksize, agentHero, agent_vil)\n",
    "rc[:]['agentHero']=agentHero\n",
    "rc[:]['agent_vil']=agent_vil\n",
    "rc[:]['weightsHero']=weightsHero\n",
    "\n",
    "#We want a diversity of opponents in the environment to prevent overfitting, and only update the weights of Villains from time to time, to keep the environment stable enough.\n",
    "#The current weights for villain are saved in the weights file.\n",
    "try:\n",
    "    with open(\"weightsVil10BB.txt\", \"rb\") as weightsFile:   # Unpickling\n",
    "        newWeights = pickle.load(weightsFile)\n",
    "    for i in range(len(newWeights)):\n",
    "        rc[i]['weightsVil']=newWeights[i]\n",
    "\n",
    "except:\n",
    "    rc[:]['weightsVil']=weightsHero\n",
    "    print(\"exception!\")\n",
    "\n",
    "\n",
    "rc[:]['ReplayBuffer2']= ReplayBuffer2\n",
    "rc[:]['env']= env\n",
    "rc[:]['nDec'] = nDec\n",
    "rc[:]['stacksize']=stacksize\n",
    "\n",
    "\n",
    "def play(instance):\n",
    "    import numpy as np\n",
    "\n",
    "    agentHero.q_eval.set_weights(weightsHero)\n",
    "    agent_vil.q_eval.set_weights(weightsVil)\n",
    "    n_rounds = 1\n",
    "    env.reset()\n",
    "\n",
    "    for i in range(n_rounds):\n",
    "        agentHero.memory = ReplayBuffer2(nDec, (107,))\n",
    "\n",
    "        while agentHero.memory.mem_cntr < nDec:\n",
    "\n",
    "            done =True\n",
    "\n",
    "            while done:\n",
    "                observation, done = env.reset()\n",
    "                hand = env.holecards\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                error = False\n",
    "                action = agentHero.choose_action(observation)\n",
    "                observation = np.append(observation,[env.pwinHero, env.plooseHero, env.pwinAvg, env.plooseAvg, env.stdWinAvg])\n",
    "\n",
    "                try: \n",
    "                    done = env.step(action) #sometimes the chosen actions lead to too many moves on a street (over 20) This happens when players both chose to engage in minraise battles.\n",
    "                                            #we want to severely punish this behavior.\n",
    "                except IndexError: #error happens when more than 20 actions are attepted on a single street.\n",
    "                    done = True\n",
    "                    error = True\n",
    "\n",
    "                observation_ = env.observation\n",
    "                observation_ = np.append(observation_,[env.pwinHero, env.plooseHero, env.pwinAvg, env.plooseAvg, env.stdWinAvg])\n",
    "                if not done:\n",
    "                    reward = 0\n",
    "                else:\n",
    "                    if error:\n",
    "                        reward = -stacksize\n",
    "                    else:\n",
    "                        reward = 0.5+env.stack_sb - env.starting_hero if env.position == 0 else 1+env.stack_bb - env.starting_hero #to take into account the blinds are posted anyway regardless of action.\n",
    "\n",
    "                agentHero.store_transition(observation, action, reward, observation_, done)\n",
    "                    #If the agent is playing against himself to learn, we can use villains play to train also. Everything except for villains hand can be derived from the observation. So we only have to save villains cards.\n",
    "\n",
    "                observation = env.observation\n",
    "\n",
    "\n",
    "        memoryDict = {'states': agentHero.memory.state_memory[:nDec], 'states_': agentHero.memory.new_state_memory[:nDec], \n",
    "                      'rewards':agentHero.memory.reward_memory[:nDec], 'actions': agentHero.memory.action_memory[:nDec], \n",
    "                      'terminals': agentHero.memory.terminal_memory[:nDec]}\n",
    "\n",
    "        agentHero.memory.mem_cntr =0\n",
    "    return memoryDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2c14166-4d5b-487d-91b8-7996131084cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0.  9.5 9.  0.  5.  7. ] [[ 0.12202423  1.0430639   0.81176305  0.7232326  -1.6350061  -0.21789956\n",
      "   0.5817083  -0.34605616  0.57846856 -0.72732997 -0.5722127  -0.83211327]]\n",
      "[ 0.  9.  9.  1.  6. 13.] [[-8.087861    1.2654951  -0.5240028   0.5784763  -0.8323055  -0.70127046\n",
      "   1.1177688  -2.2329066  -0.36103934  0.71561825 -0.48946756 -4.4032707 ]]\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 9.6373\n",
      "exchanging weights of 1\n",
      "[[ 0.13850917  0.07992314  0.06913903 ... -0.12545016  0.01711711\n",
      "   0.05705527]\n",
      " [ 0.17504795  0.03951159 -0.05156729 ...  0.00361738  0.02098753\n",
      "   0.10995629]\n",
      " [ 0.13493286 -0.06110438  0.04001645 ...  0.03141897 -0.02620837\n",
      "  -0.08020692]\n",
      " ...\n",
      " [ 0.08809458 -0.039995   -0.04733214 ...  0.03349125  0.02571337\n",
      "   0.07833067]\n",
      " [ 0.11676752 -0.10245346  0.0956275  ... -0.05567842  0.05941088\n",
      "  -0.09875693]\n",
      " [ 0.04837724  0.09834288 -0.05981735 ...  0.03141129 -0.11641587\n",
      "  -0.0100168 ]]\n",
      "[[ 0.13846341  0.08019175  0.06943158 ... -0.12571284  0.01751401\n",
      "   0.05757669]\n",
      " [ 0.17536767  0.0399639  -0.05157166 ...  0.00318196  0.01843299\n",
      "   0.10997167]\n",
      " [ 0.13553257 -0.06029432  0.04032636 ...  0.03065398 -0.028261\n",
      "  -0.07958193]\n",
      " ...\n",
      " [ 0.08829145 -0.0394924  -0.04706477 ...  0.03281712  0.02414784\n",
      "   0.07862579]\n",
      " [ 0.11706858 -0.10189701  0.09573411 ... -0.05626488  0.05746825\n",
      "  -0.09862466]\n",
      " [ 0.04854114  0.09885766 -0.05940931 ...  0.03062707 -0.11745439\n",
      "  -0.00958006]]\n",
      "1\n",
      "[0. 9. 0. 0. 5. 5.] [[-5.1125097  -1.922848    0.68291056  1.2014562  -6.728404   -2.3777292\n",
      "  -3.3141522   1.9897621  -0.39026645  0.3594458  -1.35619    -3.9886193 ]]\n",
      "[0. 9. 9. 3. 5. 7.] [[-7.9364862   0.8935997  -0.7447079   0.17949617 -1.2867062  -0.9759528\n",
      "   0.7745503  -2.8265023  -1.2791913   0.02079326 -0.16587295 -4.4865212 ]]\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 10.4414\n",
      "2\n",
      "[0. 9. 0. 0. 8. 9.] [[-5.3028364  -0.8365515  -0.13085696  0.49574393 -6.395812   -2.1978035\n",
      "  -3.0532699   1.6149567  -0.48807654  0.81924564 -2.2499907  -4.6461363 ]]\n",
      "[0.  9.5 9.  0.  4.  5. ] [[-0.02568967  1.3987579   0.01096025  0.09878617 -1.4170012  -0.54207295\n",
      "   0.3928119  -1.2643669  -0.1405083  -1.6020972  -0.78115433 -1.5351063 ]]\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 9.2099\n",
      "3\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "Result not ready.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m terminals\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nCores):\n\u001b[0;32m---> 18\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmemoryList\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m     states_\u001b[38;5;241m.\u001b[39mappend(memoryList[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates_\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     20\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(memoryList[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ipyparallel/client/asyncresult.py:698\u001b[0m, in \u001b[0;36mAsyncResult.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;124;03m\"\"\"getitem returns result value(s) if keyed by int/slice, or metadata if key is str.\"\"\"\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 698\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collect_exceptions([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult()[key]])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ipyparallel/client/asyncresult.py:381\u001b[0m, in \u001b[0;36mAsyncResult._check_ready\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_ready\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[0;32m--> 381\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult not ready.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTimeoutError\u001b[0m: Result not ready."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "nCores = 8#how many parallel cores were running.\n",
    "oldWeightsCount = 1\n",
    "\n",
    "for j in range (6000):\n",
    "    print (j)\n",
    "    memoryList = rc[:].map_sync(play,range(nCores))\n",
    "\n",
    "    states = []\n",
    "    states_=[]\n",
    "    rewards=[]\n",
    "    actions=[]\n",
    "    terminals=[]\n",
    "    \n",
    "    for i in range(nCores):\n",
    "        states.append(memoryList[i]['states'])\n",
    "        states_.append(memoryList[i]['states_'])\n",
    "        rewards.append(memoryList[i]['rewards'])\n",
    "        actions.append(memoryList[i]['actions'])\n",
    "        terminals.append(memoryList[i]['terminals'])\n",
    "    \n",
    "    state_memory=np.concatenate(states,axis=0)\n",
    "    new_state_memory = np.concatenate(states_,axis=0)\n",
    "    reward_memory = np.concatenate(rewards,axis=0)\n",
    "    action_memory = np.concatenate(actions,axis=0)\n",
    "    terminal_memory = np.concatenate(terminals,axis=0)\n",
    "    \n",
    "    agentLearn.memory.state_memory = state_memory\n",
    "    agentLearn.memory.new_state_memory = new_state_memory\n",
    "    agentLearn.memory.reward_memory = reward_memory\n",
    "    agentLearn.memory.action_memory = action_memory\n",
    "    agentLearn.memory.terminal_memory = terminal_memory\n",
    "    agentLearn.memory.mem_cntr = nCores*nDec\n",
    "    \n",
    "    r1 = np.random.randint(0, nCores*nDec)\n",
    "    r2 = np.random.randint(0, nCores*nDec)\n",
    "    observation = agentLearn.memory.state_memory[r1]\n",
    "    print(observation[0:6],agentLearn.q_eval.predict(np.array([observation])))\n",
    "    observation = agentLearn.memory.state_memory[r2]\n",
    "    print(observation[0:6],agentLearn.q_eval.predict(np.array([observation])))\n",
    "\n",
    " \n",
    "    agentLearn.learnMass()   \n",
    "    agentLearn.save_model()\n",
    "    \n",
    "    gc.collect()\n",
    "    weights = agentLearn.q_eval.get_weights()\n",
    "    rc[:]['weightsHero']=weights #send current weights of heros model to the engines.\n",
    "    rc[7]['weightsVillain']=weights #one opponent always has the current weights.\n",
    "    #update villain weights. After some steps, one of the villain networks gets updated.\n",
    "    frequencyUpdateVillain = 200\n",
    "    \n",
    "    if j%frequencyUpdateVillain ==0:\n",
    "        print(\"exchanging weights of\",oldWeightsCount)\n",
    "        print (rc[oldWeightsCount]['weightsVil'][0])\n",
    "        rc[oldWeightsCount]['weightsVil']= weights\n",
    "        print (rc[oldWeightsCount]['weightsVil'][0])\n",
    "        if oldWeightsCount == 6:\n",
    "            oldWeightsCount = 0\n",
    "        else:\n",
    "            oldWeightsCount+=1\n",
    "\n",
    "#save villain weights\n",
    "    if j%500 ==0:\n",
    "        weightsVil = rc[:]['weightsVil']\n",
    "        with open(\"weightsVil10BB.txt\", \"wb\") as weightsFile:   #Pickling\n",
    "            pickle.dump(weightsVil, weightsFile)\n",
    "\n",
    "weightsVil = rc[:]['weightsVil']\n",
    "with open(\"weightsVil10BB.txt\", \"wb\") as weightsFile:   #Pickling\n",
    "    pickle.dump(weightsVil, weightsFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfc30b3a-f83c-4a47-ac3b-3928066afbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save villain weights\n",
    "weightsVil = rc[:]['weightsVil']\n",
    "with open(\"weightsVil10BB.txt\", \"wb\") as weightsFile:   #Pickling\n",
    "    pickle.dump(weightsVil, weightsFile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
